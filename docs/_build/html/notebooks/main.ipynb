{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook \n",
    "Project Topic: Network Anomaly Detection with BitNet<br><br>\n",
    "Project Description: One of the main problems with machine learning models on a large scale is their resource-intensive nature. This can cause massive spikes in both energy usage and runtime, which is especially problematic in fields such as networking, where latency is of the utmost importance. Our goal with this project is to transfer the principles of BitNet, a recent model which does away with resource-intensive matrix multiplications via the quantization of weights, to a networking context. Specifically, we will build a simple regression model with this method, which we will then use on a dataset of packet traces during which several intrusions were attempted.\n",
    "\n",
    "Collaborators: Alexander Baumgartner, Alexander Williams, Alejandro Alonso\n",
    "\n",
    "Professor: Nick Feamster\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib scikit-learn pandas numpy scipy torch tqdm joblib pathlib psutil pyRapl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import argparse\n",
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "import matplolib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyRapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyRAPL\n",
    "    PYRAPL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYRAPL_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Implementation of the data processing class and integration function continues...\n",
    "class MemoryEfficientIDSDataProcessor:\n",
    "    \"\"\"\n",
    "    Improved data processor with better preprocessing and memory management.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.scaler = RobustScaler()  # Changed to RobustScaler for better handling of outliers\n",
    "        self.feature_stats = {}\n",
    "        self.attack_mapping = {\n",
    "            'BENIGN': 'Benign',\n",
    "            'FTP-Patator': 'Brute Force',\n",
    "            'SSH-Patator': 'Brute Force',\n",
    "            'DoS GoldenEye': 'DoS',\n",
    "            'DoS Hulk': 'DoS',\n",
    "            'DoS Slowhttptest': 'DoS',\n",
    "            'DoS slowloris': 'DoS',\n",
    "            'Heartbleed': 'Heartbleed',\n",
    "            'Web Attack - Brute Force': 'Web Attack',\n",
    "            'Web Attack - Sql Injection': 'Web Attack',\n",
    "            'Web Attack - SQL Injection': 'Web Attack',\n",
    "            'Web Attack - XSS': 'Web Attack',\n",
    "            'Infiltration': 'Infiltration',\n",
    "            'Bot': 'Bot',\n",
    "            'PortScan': 'PortScan',\n",
    "            'DDoS': 'DDoS'\n",
    "        }\n",
    "\n",
    "    def preprocess_chunk(self, chunk):\n",
    "        \"\"\"\n",
    "        Preprocess a single chunk of data with improved cleaning.\n",
    "        \"\"\"\n",
    "        # Make a copy to avoid modifying the original\n",
    "        processed_chunk = chunk.copy()\n",
    "        \n",
    "        # Get numeric columns, excluding 'Label' if it exists\n",
    "        numeric_cols = processed_chunk.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if 'Label' in numeric_cols:\n",
    "            numeric_cols.remove('Label')\n",
    "        \n",
    "        # Handle numeric columns only\n",
    "        for col in numeric_cols:\n",
    "            try:\n",
    "                # Replace inf values\n",
    "                processed_chunk[col] = processed_chunk[col].replace([np.inf, -np.inf], np.nan)\n",
    "                \n",
    "                # Calculate outlier bounds\n",
    "                q1 = processed_chunk[col].quantile(0.25)\n",
    "                q3 = processed_chunk[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 3 * iqr\n",
    "                upper_bound = q3 + 3 * iqr\n",
    "                \n",
    "                # Clip outliers\n",
    "                processed_chunk[col] = processed_chunk[col].clip(lower_bound, upper_bound)\n",
    "                \n",
    "                # Handle skewness only if the column has no NaN values\n",
    "                if not processed_chunk[col].isna().any():\n",
    "                    skewness = processed_chunk[col].skew()\n",
    "                    if abs(skewness) > 1:\n",
    "                        # Ensure all values are positive before log transform\n",
    "                        min_val = processed_chunk[col].min()\n",
    "                        if min_val < 0:\n",
    "                            processed_chunk[col] = processed_chunk[col] - min_val + 1\n",
    "                        processed_chunk[col] = np.log1p(processed_chunk[col])\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing column {col}: {str(e)}\")\n",
    "                # If there's an error processing the column, keep it as is\n",
    "                continue\n",
    "\n",
    "        return processed_chunk\n",
    "\n",
    "    def process_file_in_chunks(self, file_path, chunk_size=100000):\n",
    "        \"\"\"\n",
    "        Process file in chunks with improved error handling and monitoring.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        total_rows = 0\n",
    "        corrupted_rows = 0\n",
    "\n",
    "        try:\n",
    "            # Read CSV in chunks\n",
    "            for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                try:\n",
    "                    # Track original row count\n",
    "                    chunk_rows = len(chunk)\n",
    "                    total_rows += chunk_rows\n",
    "\n",
    "                    # Basic cleaning\n",
    "                    chunk.columns = chunk.columns.str.strip()\n",
    "                    \n",
    "                    # Preprocess chunk\n",
    "                    cleaned_chunk = self.preprocess_chunk(chunk)\n",
    "                    \n",
    "                    if not cleaned_chunk.empty:\n",
    "                        chunks.append(cleaned_chunk)\n",
    "                    else:\n",
    "                        corrupted_rows += chunk_rows\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error processing chunk: {str(e)}\")\n",
    "                    corrupted_rows += chunk_rows\n",
    "\n",
    "                # Force garbage collection\n",
    "                gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "\n",
    "        # Report statistics\n",
    "        if total_rows > 0:\n",
    "            print(f\"Processed {total_rows} total rows\")\n",
    "            print(f\"Removed {corrupted_rows} corrupted rows ({(corrupted_rows/total_rows)*100:.2f}%)\")\n",
    "\n",
    "        return pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()\n",
    "\n",
    "    def load_and_preprocess_data(self, data_dir, chunk_size=100000):\n",
    "        \"\"\"\n",
    "        Load and preprocess data with improved monitoring and validation.\n",
    "        \"\"\"\n",
    "        processed_data = []\n",
    "        total_samples = 0\n",
    "        attack_distribution = {}\n",
    "\n",
    "        # Process only Tuesday's data first\n",
    "        tuesday_file = \"Tuesday-WorkingHours.pcap_ISCX.csv\"\n",
    "        file_path = Path(data_dir) / tuesday_file\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Could not find {tuesday_file} in {data_dir}\")\n",
    "        \n",
    "        print(f\"\\nProcessing {tuesday_file}...\")\n",
    "            \n",
    "        df = self.process_file_in_chunks(file_path, chunk_size)\n",
    "        if not df.empty:\n",
    "            # Track attack distribution\n",
    "            if 'Label' in df.columns:\n",
    "                attack_counts = df['Label'].value_counts()\n",
    "                for attack, count in attack_counts.items():\n",
    "                    attack_distribution[attack] = attack_distribution.get(attack, 0) + count\n",
    "                total_samples += len(df)\n",
    "            \n",
    "            processed_data.append(df)\n",
    "            \n",
    "        gc.collect()\n",
    "\n",
    "        # Print data statistics\n",
    "        print(\"\\nData Statistics:\")\n",
    "        print(f\"Total samples: {total_samples}\")\n",
    "        print(\"\\nAttack distribution:\")\n",
    "        for attack, count in attack_distribution.items():\n",
    "            percentage = (count/total_samples)*100\n",
    "            print(f\"{attack}: {count} samples ({percentage:.2f}%)\")\n",
    "\n",
    "        # Combine processed data (just Tuesday in this case)\n",
    "        print(\"\\nCombining processed data...\")\n",
    "        full_data = processed_data[0]  # Take only Tuesday's data\n",
    "        del processed_data\n",
    "        gc.collect()\n",
    "\n",
    "        if full_data.empty:\n",
    "            raise ValueError(\"No data was successfully processed\")\n",
    "\n",
    "        # Encode labels\n",
    "        print(\"Encoding labels...\")\n",
    "        full_data['Attack_Category'] = full_data['Label'].replace(self.attack_mapping)\n",
    "        full_data['Attack_Category'] = full_data['Attack_Category'].fillna('Unknown')\n",
    "        full_data['Label_Binary'] = (full_data['Attack_Category'] != 'Benign').astype(np.float32)\n",
    "\n",
    "        # Select features\n",
    "        feature_columns = full_data.select_dtypes(include=[np.number]).columns\n",
    "        feature_columns = feature_columns.drop(['Label_Binary'])\n",
    "\n",
    "        # Extract features and handle NaN values\n",
    "        print(\"Handling missing values in features...\")\n",
    "        X = full_data[feature_columns].values\n",
    "        \n",
    "        # Fill NaN values with column medians\n",
    "        for col_idx in range(X.shape[1]):\n",
    "            col_median = np.nanmedian(X[:, col_idx])\n",
    "            mask = np.isnan(X[:, col_idx])\n",
    "            X[mask, col_idx] = col_median\n",
    "        \n",
    "        y = full_data['Label_Binary'].values\n",
    "\n",
    "        # Verify no NaN values remain\n",
    "        assert not np.isnan(X).any(), \"NaN values remain after median filling\"\n",
    "        assert not np.isnan(y).any(), \"NaN values found in labels\"\n",
    "\n",
    "        # Store feature statistics\n",
    "        self.feature_stats = {\n",
    "            'columns': feature_columns,\n",
    "            'means': np.mean(X, axis=0),\n",
    "            'stds': np.std(X, axis=0),\n",
    "            'mins': np.min(X, axis=0),\n",
    "            'maxs': np.max(X, axis=0)\n",
    "        }\n",
    "\n",
    "        print(f\"Final dataset shape: {X.shape}\")\n",
    "        print(f\"Number of features: {len(feature_columns)}\")\n",
    "        print(f\"Class distribution: {np.bincount(y.astype(int))}\")\n",
    "\n",
    "        return X, y, feature_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
