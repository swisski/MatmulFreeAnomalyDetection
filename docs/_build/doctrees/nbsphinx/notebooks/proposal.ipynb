{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal\n",
    "\n",
    "Project Title: Network Anomaly Detection with BitNet\n",
    "\n",
    "Project Participants:\n",
    "\n",
    "| Name | Surname | CNET ID | Project Role |\n",
    "|------|----------|----------|--------------|\n",
    "| Alexander | Baumgartner | abaumgartner | Model development and testing |\n",
    "| Alexander | Williams | agwilliams200 | Quantization and model training |\n",
    "| Alejandro | Alonso | aalonso20 | Model testing and writeup |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "One of the main problems with machine learning models on a large scale is their resource-intensive nature. This can cause massive spikes in both energy usage and runtime, which is especially problematic in fields such as networking, where latency is of the utmost importance. Our goal with this project is to transfer the principles of BitNet, a recent model which does away with resource-intensive matrix multiplications via the quantization of weights, to a networking context. Specifically, we will build a simple regression model with this method, which we will then use on a dataset of packet traces during which several intrusions were attempted. Given enough time, we will experiment with extending the methodology to more complex machine learning models in an effort to maximize our accuracy and efficiency. Our hope is that our model can quickly and accurately predict when an anomaly or intrusion is occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Models\n",
    "\n",
    "1. **Supervised Binary-Weight Temporal Classifier**\n",
    "   - Adapts BitNet's 1-bit weight architecture but adds temporal convolution layers to process network packet windows\n",
    "   - Uses labeled anomaly data to train a binary classifier (normal/anomaly), with the model maintaining BitNet's memory efficiency while learning from known attack patterns\n",
    "   - Strategy: Train on sliding windows of network metrics where some days contain labeled anomalies, using BitNet's group quantization for efficient batch processing\n",
    "\n",
    "2. **Unsupervised BitNet Autoencoder**\n",
    "   - Implements BitNet's binary weight approach in an autoencoder architecture to learn normal network behavior patterns\n",
    "   - Detects anomalies by measuring reconstruction error between input and output traffic patterns\n",
    "   - Strategy: Train only on normal traffic days to establish baseline behavior, then flag deviations during inference, using BitNet's efficient memory footprint to process longer historical windows\n",
    "\n",
    "3. **Semi-Supervised Hybrid Detector**\n",
    "   - Combines a BitNet-based supervised classifier for known attack patterns with an unsupervised component for detecting novel anomalies\n",
    "   - Uses binary weights in both components but maintains separate detection pathways\n",
    "   - Strategy: Leverage labeled data where available while still being able to detect unknown anomalies, using BitNet's scaling properties to handle the larger dual architecture efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The project will utilize the Network Intrusion Dataset available at:\n",
    "[Network Intrusion Dataset on Kaggle](https://www.kaggle.com/datasets/chethuhn/network-intrusion-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "A Jupyter notebook containing:\n",
    "\n",
    "1. Original regression model without quantized weights\n",
    "2. Performance statistics for the non-quantized model:\n",
    "   - Confusion matrix evaluated using an 80/20 train-test split\n",
    "   - Timing measurements\n",
    "   - CPU/GPU usage statistics\n",
    "3. Mathematical documentation:\n",
    "   - Weight quantization process\n",
    "   - Implementation of quantized weights for row operations instead of matmul operations\n",
    "4. Performance statistics for the quantized model:\n",
    "   - Confusion matrix evaluated using an 80/20 train-test split\n",
    "   - Timing measurements\n",
    "   - CPU/GPU usage statistics\n",
    "5. A brief writeup (Sphinx) detailing:\n",
    "   - Encountered obstacles\n",
    "   - Model performance analysis\n",
    "   - Success evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
